domain:
  name: Hopper-v4
  total_timesteps: 2000000 # how many learn steps the agent should take
  #episodes: 1000
  algo: TD3 # Valid values: 'DDPG', 'TD3'
  init_learn_timestep: 25001 # at which timestep should the agent start learning
  #learning_starts_ep: 10 # Start Learning at episode X, before that fill the ERB with random actions
  evaluation_frequency: 5000 # how ofter should
  runs: 10 # number of statistical runs
  seed: 64 # seeds the enviroment
DDPG:
  gamma: 0.99 # Reward Discount rate
  #gamma: 1
  tau: 0.01 # Target Network Update rate
  N: 100 # Experience Replay Buffer's mini match size
  experience_replay_buffer_size: 1000000
  sigma: 0.1 # standard deviation of the action process for exploration
  optimizer_gamma: 0.001 # the learning rate of the optimizers
  mu_bias: True # Bias for the actor module
  q_bias: True # Bias for the critic module
TD3:
  gamma: 0.99 # Reward Discount rate
  #gamma: 1
  tau: 0.005 # Target Network Update rate
  N: 256 # Experience Replay Buffer's mini match size
  experience_replay_buffer_size: 1000000
  sigma_policy: 0.2 # Standard deviation of the action process for policy update
  sigma_explore: 0.1 # Standard deviation of the action process for exploration
  optimizer_gamma: 0.001 # The learning rate of the optimizers
  noise_policy_clip: 0.5 # Clamping for the target noise
  d: 2 # Policy Update Frequency
  mu_bias: True # Bias for the actor module
  q_bias: True # Bias for the critics module